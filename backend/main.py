import time
import json
import redis
import requests
import logging
from flask import Flask, jsonify, request
from datetime import datetime
from collections import defaultdict

logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

app = Flask(__name__)
redis_client = redis.Redis(host="localhost", port=6379, db=0, decode_responses=True)

HEADERS = {
    "Content-Type": "application/json",
    "system-name": "Prefektura_UVAO",
    "system-password": "78ldU(159dem91w5(w!wx"
}

URL_FIND = "https://api-gorod.mos.ru/api/issues/issue/find"
URL_GET = "https://api-gorod.mos.ru/api/issues/issue/get"


def fetch_full_issue_data(issue_ids):
    payload = {
        "ids": issue_ids,
        "expand": ["comments", "comments.attachments", "object",
                   "comments.monitor", "comments.monitor.violation_name",
                   "comments.monitor.element_2"]
    }
    response = requests.post(URL_GET, json=payload, headers=HEADERS, verify=False)

    if response.status_code == 200:
        return response.json().get("data", [])
    else:
        logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–∞–Ω–Ω—ã—Ö ({response.status_code}): {response.text}")
        return []

def update_cache():
    while True:
        logging.info("üîÑ –ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö —Å –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞...")

        all_issue_ids = []
        offset = 0
        limit = 500

        while True:
            logging.info(f"üì° –ó–∞–ø—Ä–æ—Å —Å offset={offset}...")
            payload = {
                "filter": {
                    "condition": [
                        "and",
                        ["=", "assignments.role_code", "oiv1"],
                        ["in", "assignments.organization_id",
                         [9150]], ##
                        ["or",
                         ["=", "public_status.title", "–ü—Ä–æ–±–ª–µ–º–∞ –≤ —Ä–∞–±–æ—Ç–µ"],
                         ]
                    ],
                    "limit": limit,
                    "offset": offset
                }
            }

            response = requests.post(URL_FIND, json=payload, headers=HEADERS, verify=False)

            if response.status_code == 200:
                data = response.json()
                issue_ids = data.get("data", {}).get("items", [])

                if not issue_ids:
                    logging.info("‚úÖ –í—Å–µ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, –¥–∞–ª—å–Ω–µ–π—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç.")
                    break

                all_issue_ids.extend(issue_ids)
                logging.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(issue_ids)} ID (–≤—Å–µ–≥–æ: {len(all_issue_ids)})")

                offset += 500

            else:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ ({response.status_code}): {response.text}")
                break

        if all_issue_ids:
            logging.info(f"üü¢ –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(all_issue_ids)} –∑–∞–ø–∏—Å–µ–π...")

            detailed_data = fetch_full_issue_data(all_issue_ids)

            if detailed_data:
                redis_client.set("cached_issues", json.dumps(detailed_data))
                redis_client.expire("cached_issues", 600)
                logging.info(f"‚úÖ –ö–µ—à –æ–±–Ω–æ–≤–ª–µ–Ω ({len(detailed_data)} –∑–∞–ø–∏—Å–µ–π).")
            else:
                logging.warning("‚ö†Ô∏è –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–ª—É—á–µ–Ω—ã.")

        else:
            logging.warning("‚ö†Ô∏è –ù–µ—Ç –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è.")

        logging.info("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ 5 –º–∏–Ω—É—Ç –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è...")
        time.sleep(600)

def extract_monitor_deadline(comments):
    if isinstance(comments, list):
        for comment in comments:
            monitor = comment.get("monitor")
            if monitor and "deadline_at" in monitor:
                return monitor["deadline_at"]
    return None

@app.route("/chart_data", methods=["GET"])
def chart_data():
    cached_data = redis_client.get("cached_issues")

    if not cached_data:
        return jsonify({"error": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ –∫–µ—à–µ"}), 503

    issues = json.loads(cached_data)

    grouped = defaultdict(int)

    for issue in issues:
        comments = issue.get("comments", [])
        deadline_str = extract_monitor_deadline(comments)
        if not deadline_str:
            continue

        try:
            deadline = datetime.fromisoformat(deadline_str)
            date_key = deadline.strftime("%Y-%m-%d")
            grouped[date_key] += 1
        except ValueError:
            continue

    result = [{"date": date, "count": count} for date, count in sorted(grouped.items())]
    return jsonify(result)

@app.route("/find_issue", methods=["GET"])
def find_issue():
    logging.info("–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å –≤ /find_issue")

    issue_id = request.args.get("issue_id")
    logging.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞: issue_id={issue_id}")

    if not issue_id:
        logging.warning("‚ö†Ô∏è –ù–µ —É–∫–∞–∑–∞–Ω issue_id –≤ –∑–∞–ø—Ä–æ—Å–µ!")
        return jsonify({"error": "–ù–µ —É–∫–∞–∑–∞–Ω issue_id"}), 400

    try:
        issue_id = int(issue_id)
    except ValueError:
        logging.warning(f"‚ö†Ô∏è –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ issue_id: {issue_id}")
        return jsonify({"error": "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π issue_id"}), 400

    logging.info(f"–ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö –ø–æ issue_id={issue_id} –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–µ—Ä")

    payload = {
        "ids": [issue_id],
        "expand": ["comments", "comments.attachments", "object",
                   "comments.monitor", "comments.monitor.violation_name",
                   "comments.monitor.element_2"]
    }

    logging.debug(f"–û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –Ω–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–µ—Ä: {URL_GET}")
    logging.debug(f"Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}")

    try:
        response = requests.post(URL_GET, json=payload, headers=HEADERS, verify=False)

        logging.info(f"–û—Ç–≤–µ—Ç –æ—Ç –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Å–µ—Ä–≤–µ—Ä–∞: —Å—Ç–∞—Ç—É—Å {response.status_code}")

        if response.status_code == 200:
            try:
                data = response.json().get("data", [])
                logging.debug(f"üìú –î–∞–Ω–Ω—ã–µ –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞: {json.dumps(data, indent=2, ensure_ascii=False)}")

                if data:
                    logging.info(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –ø–æ issue_id={issue_id}")
                    return jsonify(data[0])
                else:
                    logging.warning(f"‚ùå –û–±—Ä–∞—â–µ–Ω–∏–µ {issue_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.")
                    return jsonify({"error": "–û–±—Ä–∞—â–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"}), 404
            except json.JSONDecodeError:
                logging.error(f"‚ùå –û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON! –û—Ç–≤–µ—Ç: {response.text}")
                return jsonify({"error": "–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –æ—Ç —Å–µ—Ä–≤–µ—Ä–∞"}), 500
        else:
            logging.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ ({response.status_code}): {response.text}")
            return jsonify({"error": f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: {response.status_code}"}), response.status_code

    except requests.RequestException as e:
        logging.exception("–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è HTTP-–∑–∞–ø—Ä–æ—Å–∞")
        return jsonify({"error": "–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–µ—Ä–≤–µ—Ä—É"}), 500

@app.route("/get_issues", methods=["GET"])
def get_issues():
    cached_data = redis_client.get("cached_issues")

    if cached_data:
        logging.info("üü¢ –î–∞–Ω–Ω—ã–µ –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ –∫–µ—à–∞.")
        return jsonify(json.loads(cached_data))
    else:
        logging.warning("üî¥ –î–∞–Ω–Ω—ã—Ö –≤ –∫–µ—à–µ –Ω–µ—Ç.")
        return jsonify({"error": "–î–∞–Ω–Ω—ã–µ –µ—â—ë –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã"}), 503


if __name__ == "__main__":
    from threading import Thread

    cache_updater = Thread(target=update_cache, daemon=True)
    cache_updater.start()

    logging.info("üöÄ –°–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É 5000")
    app.run(host="0.0.0.0", port=5000, debug=True)
